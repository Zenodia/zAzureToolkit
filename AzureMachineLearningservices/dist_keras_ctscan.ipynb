{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.6\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      #'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "# only need to do it once\n",
    "ds2 = Datastore.register_azure_file_share(workspace=ws, \n",
    "                                         datastore_name='choose_a_datastore_name', \n",
    "                                         file_share_name='your_fileshare_name',\n",
    "                                         account_name='your_storage_acc_name', \n",
    "                                         account_key='your_storage_acc_key',\n",
    "                                         create_if_not_exists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n",
      "{'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-01-09T14:14:16.649000+00:00', 'creationTime': '2019-01-08T10:23:55.033355+00:00', 'currentNodeCount': 0, 'errors': None, 'modifiedTime': '2019-01-08T10:25:35.793472+00:00', 'nodeStateCounts': {'idleNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0, 'preparingNodeCount': 0, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'targetNodeCount': 0, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './dist-keras-ctscan-exp'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dist-keras-ctscan-exp/dist_keras_ctscan.py'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "#shutil.copy('keras_cnn_dicom.py', project_folder)\n",
    "shutil.copy('dist_keras_ctscan.py', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'dist-keras-tf-exp'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r",
      "\r\n",
      "import argparse\r",
      "\r\n",
      "import pydicom\r",
      "\r\n",
      "from matplotlib import pyplot, cm\r",
      "\r\n",
      "import os\r",
      "\r\n",
      "import sys\r",
      "\r\n",
      "import numpy as np\r",
      "\r\n",
      "import pandas as pd\r",
      "\r\n",
      "import scipy\r",
      "\r\n",
      "import keras\r",
      "\r\n",
      "from keras.models import Sequential\r",
      "\r\n",
      "from keras.layers import AveragePooling2D , Convolution2D , Flatten ,Dense, MaxPooling2D, Conv2D\r",
      "\r\n",
      "from keras.preprocessing import utils\r",
      "\r\n",
      "from keras.preprocessing.image import ImageDataGenerator\r",
      "\r\n",
      "from keras import backend as K\r",
      "\r\n",
      "import math\r",
      "\r\n",
      "import tensorflow as tf\r",
      "\r\n",
      "import horovod.keras as hvd\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "# Horovod: initialize Horovod.\r",
      "\r\n",
      "hvd.init()\r",
      "\r\n",
      "\r",
      "\r\n",
      "# Horovod: pin GPU to be used to process local rank (one GPU per process)\r",
      "\r\n",
      "config = tf.ConfigProto()\r",
      "\r\n",
      "config.gpu_options.allow_growth = True\r",
      "\r\n",
      "config.gpu_options.visible_device_list = str(hvd.local_rank())\r",
      "\r\n",
      "K.set_session(tf.Session(config=config))\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "# Horovod: adjust number of epochs based on number of GPUs.\r",
      "\r\n",
      "epochs = 1\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "def get_data(dicom_dir):\r",
      "\r\n",
      "    #resize the image to desired resolution\r",
      "\r\n",
      "    #print(\"dicom_dir\",os.listdir(dicom_dir), dicom_dir)\r",
      "\r\n",
      "    xsize = 256; ysize = 256\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    data = np.zeros((xsize, ysize, 100))\r",
      "\r\n",
      "    #print(\"dicom_dir\",os.listdir(dicom_dir), dicom_dir)\r",
      "\r\n",
      "    for i, s in enumerate(os.listdir(dicom_dir)):\r",
      "\r\n",
      "    \r",
      "\r\n",
      "        img = np.array(pydicom.read_file(dicom_dir+ s).pixel_array)\r",
      "\r\n",
      "        xscale = xsize/img.shape[0]\r",
      "\r\n",
      "        yscale = ysize/img.shape[1]\r",
      "\r\n",
      "        data[:,:,i] = scipy.ndimage.interpolation.zoom(img, [xscale, yscale])\r",
      "\r\n",
      "    #returning a numpy array of shape 100,256,256\r",
      "\r\n",
      "    return data\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "if __name__=='__main__': \r",
      "\r\n",
      "    \r",
      "\r\n",
      "    parser=argparse.ArgumentParser()\r",
      "\r\n",
      "    parser.add_argument('-i','--data',help='directory of where dicom files exists' )\r",
      "\r\n",
      "    parser.add_argument('--epoch', help='how many epoch to train on')\r",
      "\r\n",
      "    #parser.add_argument('--reload', help='path to where you save the previous model and use it to continue training')\r",
      "\r\n",
      "    parser.add_argument('--save_model', help='path to where you want to save the model')\r",
      "\r\n",
      "    args=parser.parse_args()\r",
      "\r\n",
      "    os.makedirs(args.data,exist_ok=True)\r",
      "\r\n",
      "    os.makedirs(args.save_model,exist_ok=True)\r",
      "\r\n",
      "    print(os.path.expandvars(args.data))\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    X=get_data(args.data+'/ChestCTscan/dicom/')\r",
      "\r\n",
      "    X=np.moveaxis(X, -1, 0)\r",
      "\r\n",
      "    print(\"check the dicom file shape should be 100, 256,256\", X.shape)\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    ### get label 1=contrast / 0=no contrast \r",
      "\r\n",
      "    df=pd.read_csv(args.data +'/ChestCTscan/overview.csv',encoding='utf-8',sep=',')\r",
      "\r\n",
      "    del df['Unnamed: 0']\r",
      "\r\n",
      "    y=df.iloc[:,1].values\r",
      "\r\n",
      "    y= np.array([1 if yi else 0 for yi in y])\r",
      "\r\n",
      "    from sklearn.model_selection import train_test_split\r",
      "\r\n",
      "    X_train, X_test, y_train,y_test=train_test_split(X,y , test_size=0.1 , random_state=0)\r",
      "\r\n",
      "    ## need to add a fake dimension in the end since keras image_generator expect 4 dim \r",
      "\r\n",
      "    X_train=np.expand_dims(X_train, axis=3)\r",
      "\r\n",
      "    X_test=np.expand_dims(X_test,axis=3)\r",
      "\r\n",
      "    X=np.expand_dims(X,axis=3)\r",
      "\r\n",
      "    X_train.shape, X_test.shape , y_train.shape, y_test.shape\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    train_datagen = ImageDataGenerator(rescale = 1./255,\r",
      "\r\n",
      "                                   shear_range = 0.2,\r",
      "\r\n",
      "                                   zoom_range = 0.2,\r",
      "\r\n",
      "                                   horizontal_flip = True)\r",
      "\r\n",
      "\r",
      "\r\n",
      "    test_datagen = ImageDataGenerator(rescale = 1./255,\r",
      "\r\n",
      "                                   shear_range = 0.2,\r",
      "\r\n",
      "                                   zoom_range = 0.2,\r",
      "\r\n",
      "                                   horizontal_flip = True)\r",
      "\r\n",
      "    train_datagen.fit(X_train, augment=True, seed=123)\r",
      "\r\n",
      "    test_datagen.fit(X_test, augment=True, seed=123)\r",
      "\r\n",
      "    train_batch=train_datagen.flow(X, y, batch_size=20, seed=123, shuffle=True )\r",
      "\r\n",
      "    test_batch=test_datagen.flow(X_test, y_test, batch_size=20, seed=123, shuffle=True )\r",
      "\r\n",
      "    # Initialising the CNN\r",
      "\r\n",
      "    classifier = Sequential()\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    # Step 1 - Convolution\r",
      "\r\n",
      "    classifier.add(Conv2D(32, (3, 3), input_shape = (256, 256 ,1), activation = 'relu', padding=\"same\"))\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    # Step 2 - Pooling\r",
      "\r\n",
      "    classifier.add(MaxPooling2D(pool_size = (2, 2)))\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    # Adding a second convolutional layer\r",
      "\r\n",
      "    classifier.add(Conv2D(64, 3, 3, activation = 'relu'))\r",
      "\r\n",
      "    classifier.add(MaxPooling2D(pool_size = (2, 2)))\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    # Adding a second convolutional layer\r",
      "\r\n",
      "    classifier.add(Conv2D(128, 3, 3, activation = 'relu'))\r",
      "\r\n",
      "    classifier.add(MaxPooling2D(pool_size = (2, 2)))\r",
      "\r\n",
      "    \r",
      "\r\n",
      "        \r",
      "\r\n",
      "    # Adding a second convolutional layer\r",
      "\r\n",
      "    classifier.add(Conv2D(256, 3, 3, activation = 'relu'))\r",
      "\r\n",
      "    classifier.add(MaxPooling2D(pool_size = (2, 2)))\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    # Step 3 - Flattening\r",
      "\r\n",
      "    classifier.add(Flatten())\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    # Step 4 - Full connection\r",
      "\r\n",
      "    classifier.add(Dense(units = 128, activation = 'relu')) # the output_dim is chosen by experience\r",
      "\r\n",
      "    classifier.add(Dense(units = 1, activation = 'sigmoid'))\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    # Horovod: adjust learning rate based on number of GPUs.\r",
      "\r\n",
      "    opt = keras.optimizers.Adadelta(1.0 * hvd.size())\r",
      "\r\n",
      "\r",
      "\r\n",
      "    # Horovod: add Horovod Distributed Optimizer.\r",
      "\r\n",
      "    opt = hvd.DistributedOptimizer(opt)\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    \r",
      "\r\n",
      "    classifier.compile(loss=keras.losses.binary_crossentropy,\r",
      "\r\n",
      "                  optimizer=opt,\r",
      "\r\n",
      "                  metrics=['accuracy'])\r",
      "\r\n",
      "\r",
      "\r\n",
      "    callbacks = [\r",
      "\r\n",
      "        # Horovod: broadcast initial variable states from rank 0 to all other processes.\r",
      "\r\n",
      "        # This is necessary to ensure consistent initialization of all workers when\r",
      "\r\n",
      "        # training is started with random weights or restored from a checkpoint.\r",
      "\r\n",
      "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\r",
      "\r\n",
      "    ]\r",
      "\r\n",
      "\r",
      "\r\n",
      "    \r",
      "\r\n",
      "    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\r",
      "\r\n",
      "    if hvd.rank() == 0:\r",
      "\r\n",
      "        callbacks.append(keras.callbacks.ModelCheckpoint(args.save_model+'/checkpoint.h5'))\r",
      "\r\n",
      "\r",
      "\r\n",
      "    classifier.fit(X_train, y_train,\r",
      "\r\n",
      "              batch_size=100,\r",
      "\r\n",
      "              callbacks=callbacks,\r",
      "\r\n",
      "              epochs=1,\r",
      "\r\n",
      "              verbose=1,\r",
      "\r\n",
      "              validation_data=(X_test, y_test))\r",
      "\r\n",
      "    score = classifier.evaluate(X_test, y_test, verbose=0)    \r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "cat ./dist-keras-ctscan-exp/dist_keras_ctscan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import *\n",
    "script_params={\n",
    "    '--data': ds2.path(),\n",
    "    '--epoch': 1,\n",
    "    '--save_model':'/outputs'\n",
    "}\n",
    "\n",
    "estimator = Estimator(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      entry_script='dist_keras_ctscan.py',\n",
    "                      script_params=script_params,\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_backend='mpi',    \n",
    "                      pip_packages=['pydicom','tensorflow-gpu', 'keras', 'horovod','scikit-image','scikit-learn','scipy','argparse',\n",
    "                                    'opencv-contrib-python-headless','pillow','numpy', 'pandas','matplotlib'],\n",
    "                      #custom_docker_base_image='zecharpy/tfgpupy3:pydicom',\n",
    "                      use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: dist-keras-tf-exp,\n",
      "Id: dist-keras-tf-exp_1547628647461,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Queued)\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(estimator)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb5002350fa4fd69163f1a66234fce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: dist-keras-tf-exp_1547628647461\n",
      "\n",
      "Streaming azureml-logs/60_control_log_rank_0.txt\n",
      "================================================\n",
      "\n",
      "This is an MPI job. Rank:0\n",
      "Streaming log file azureml-logs/60_control_log_rank_0.txt\n",
      "Streaming log file azureml-logs/80_driver_log_rank_0.txt\n",
      "\n",
      "Streaming azureml-logs/80_driver_log_rank_1.txt\n",
      "===============================================\n",
      "\n",
      "Using TensorFlow backend.\n",
      "\n",
      "Streaming azureml-logs/80_driver_log_rank_0.txt\n",
      "===============================================\n",
      "\n",
      "Using TensorFlow backend.\n",
      "2019-01-16 09:00:31.498467: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-01-16 09:00:31.620676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 724f:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-01-16 09:00:31.620718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
      "2019-01-16 09:00:31.903127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-01-16 09:00:31.903184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
      "2019-01-16 09:00:31.903195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
      "2019-01-16 09:00:31.903476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 724f:00:00.0, compute capability: 3.7)\n",
      "/mnt/batch/tasks/shared/LS_root/jobs/zaml/azureml/dist-keras-tf-exp_1547628647461/mounts/ctscands\n",
      "check the dicom file shape should be 100, 256,256 (100, 256, 256)\n",
      "dist_keras_ctscan.py:111: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  classifier.add(Conv2D(64, 3, 3, activation = 'relu'))\n",
      "dist_keras_ctscan.py:115: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "  classifier.add(Conv2D(128, 3, 3, activation = 'relu'))\n",
      "dist_keras_ctscan.py:120: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "  classifier.add(Conv2D(256, 3, 3, activation = 'relu'))\n",
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/1\n",
      "\n",
      "90/90 [==============================] - 4s 45ms/step - loss: 3.1047 - acc: 0.6667 - val_loss: 11.1597 - val_acc: 0.3000\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "1 items cleaning up...\n",
      "Cleanup took 0.1006004810333252 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: dist-keras-tf-exp_1547628647461\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'dist-keras-tf-exp_1547628647461',\n",
       " 'target': 'gpucluster',\n",
       " 'status': 'Finalizing',\n",
       " 'startTimeUtc': '2019-01-16T08:58:06.105893Z',\n",
       " 'properties': {'azureml.runsource': 'experiment',\n",
       "  'ContentSnapshotId': '69689924-e351-4896-87b5-4b7d99a58877'},\n",
       " 'runDefinition': {'Script': 'dist_keras_ctscan.py',\n",
       "  'Arguments': ['--data',\n",
       "   '$AZUREML_DATAREFERENCE_ctscands',\n",
       "   '--epoch',\n",
       "   '1',\n",
       "   '--save_model',\n",
       "   '/outputs'],\n",
       "  'SourceDirectoryDataStore': None,\n",
       "  'Framework': 0,\n",
       "  'Communicator': 5,\n",
       "  'Target': 'gpucluster',\n",
       "  'DataReferences': {'ctscands': {'DataStoreName': 'ctscands',\n",
       "    'Mode': 'Mount',\n",
       "    'PathOnDataStore': None,\n",
       "    'PathOnCompute': None,\n",
       "    'Overwrite': False}},\n",
       "  'JobName': None,\n",
       "  'AutoPrepareEnvironment': True,\n",
       "  'MaxRunDurationSeconds': None,\n",
       "  'NodeCount': 2,\n",
       "  'Environment': {'Python': {'InterpreterPath': 'python',\n",
       "    'UserManagedDependencies': False,\n",
       "    'CondaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults',\n",
       "        'pydicom',\n",
       "        'tensorflow-gpu',\n",
       "        'keras',\n",
       "        'horovod',\n",
       "        'scikit-image',\n",
       "        'scikit-learn',\n",
       "        'scipy',\n",
       "        'argparse',\n",
       "        'opencv-contrib-python-headless',\n",
       "        'pillow',\n",
       "        'numpy',\n",
       "        'pandas',\n",
       "        'matplotlib']}]}},\n",
       "   'EnvironmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE',\n",
       "    'NCCL_SOCKET_IFNAME': '^docker0'},\n",
       "   'Docker': {'BaseImage': 'mcr.microsoft.com/azureml/base-gpu:0.2.0',\n",
       "    'Enabled': True,\n",
       "    'SharedVolumes': True,\n",
       "    'Preparation': None,\n",
       "    'GpuSupport': True,\n",
       "    'ShmSize': '1g',\n",
       "    'Arguments': [],\n",
       "    'BaseImageRegistry': {'Address': None,\n",
       "     'Username': None,\n",
       "     'Password': None}},\n",
       "   'Spark': {'Repositories': ['https://mmlspark.azureedge.net/maven'],\n",
       "    'Packages': [{'Group': 'com.microsoft.ml.spark',\n",
       "      'Artifact': 'mmlspark_2.11',\n",
       "      'Version': '0.12'}],\n",
       "    'PrecachePackages': True}},\n",
       "  'History': {'OutputCollection': True},\n",
       "  'Spark': {'Configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'BatchAi': {'NodeCount': 0},\n",
       "  'AmlCompute': {'Name': None,\n",
       "   'VmSize': None,\n",
       "   'VmPriority': None,\n",
       "   'RetainCluster': False,\n",
       "   'ClusterMaxNodeCount': 2},\n",
       "  'Tensorflow': {'WorkerCount': 1, 'ParameterServerCount': 1},\n",
       "  'Mpi': {'ProcessCountPerNode': 1},\n",
       "  'Hdi': {'YarnDeployMode': 2},\n",
       "  'ContainerInstance': {'Region': None, 'CpuCores': 0, 'MemoryGb': 0},\n",
       "  'ExposedPorts': None,\n",
       "  'PrepareEnvironment': None},\n",
       " 'logFiles': {'azureml-logs/60_control_log_rank_0.txt': 'https://zaml5744793264.blob.core.windows.net/azureml/ExperimentRun/dcid.dist-keras-tf-exp_1547628647461/azureml-logs/60_control_log_rank_0.txt?sv=2018-03-28&sr=b&sig=BpGbya4I%2BlTyBQgYmSQ%2BGKnya1yXyYSENspydSF84L4%3D&st=2019-01-16T08%3A53%3A41Z&se=2019-01-16T17%3A03%3A41Z&sp=r',\n",
       "  'azureml-logs/60_control_log_rank_1.txt': 'https://zaml5744793264.blob.core.windows.net/azureml/ExperimentRun/dcid.dist-keras-tf-exp_1547628647461/azureml-logs/60_control_log_rank_1.txt?sv=2018-03-28&sr=b&sig=phirR7sbN75MzmMeA7O9Nmsk0yg2EwTLGceYFQL%2FlNY%3D&st=2019-01-16T08%3A53%3A41Z&se=2019-01-16T17%3A03%3A41Z&sp=r',\n",
       "  'azureml-logs/80_driver_log_rank_1.txt': 'https://zaml5744793264.blob.core.windows.net/azureml/ExperimentRun/dcid.dist-keras-tf-exp_1547628647461/azureml-logs/80_driver_log_rank_1.txt?sv=2018-03-28&sr=b&sig=UpF02BAocb6fH7xQXpFoTWMwStmRQxT62yv6To7wyGE%3D&st=2019-01-16T08%3A53%3A41Z&se=2019-01-16T17%3A03%3A41Z&sp=r',\n",
       "  'azureml-logs/80_driver_log_rank_0.txt': 'https://zaml5744793264.blob.core.windows.net/azureml/ExperimentRun/dcid.dist-keras-tf-exp_1547628647461/azureml-logs/80_driver_log_rank_0.txt?sv=2018-03-28&sr=b&sig=1MzyRlHYtzgplsUEr1rkaOe341DhGdYbhqSTOq%2F62UA%3D&st=2019-01-16T08%3A53%3A41Z&se=2019-01-16T17%3A03%3A41Z&sp=r',\n",
       "  'azureml-logs/azureml.log': 'https://zaml5744793264.blob.core.windows.net/azureml/ExperimentRun/dcid.dist-keras-tf-exp_1547628647461/azureml-logs/azureml.log?sv=2018-03-28&sr=b&sig=ZHeTN2V4nWEFYTfQVY4oB5h%2FSPdaG9BnzqnAKL0tg%2Fk%3D&st=2019-01-16T08%3A53%3A41Z&se=2019-01-16T17%3A03%3A41Z&sp=r'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
